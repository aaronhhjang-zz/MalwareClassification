import warnings

import pandas as pd
import os
import glob

import scipy
from sklearn.preprocessing import LabelEncoder
from sklearn import tree
import numpy as np
import dataPreprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV

path_to_json = r".\simplifiedReports\\"

def get_score(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    print("=============SCORE OF " + str(model) + " ==================")
    print(model.score(X_test, y_test))
    #return model.score(X_test, y_test)
    if(hasattr(model, "best_params_")):
        print("==========values from best_params and best_score==================")
        print("Tuned Params {}".format(model.best_params_))
        print("Best score is {}".format(model.best_score_))

#read all json files into pandas dataframe
json_pattern = os.path.join(path_to_json,'*.json')
file_list = glob.glob(json_pattern)
dfs = [] # an empty list to store the data frames
for file in file_list:
    data = pd.read_json(file, lines=True) # read data frame from json file
    dfs.append(data) # append the data frame to the list
df = pd.concat(dfs, ignore_index=True) # concatenate all the data frames in the list.

#making "inputs" dataframe
#split data into inputs and target (X and Y)
inputs = df.drop(['signatures','family'], axis='columns')
#inputs = inputs.drop('family', axis='columns')
target = df['family']

index = 0
for feature in dataPreprocessing.FEATURES:
    inputs[feature] = dataPreprocessing.transposedFeatureMatrix[index]
    index += 1


# Encode data
le_name = LabelEncoder()
le_score = LabelEncoder()
inputs['nameNEW'] = le_name.fit_transform(inputs['name'])
inputs['scoreNEW'] = le_score.fit_transform(inputs['score'])
inputs = inputs.drop(["name", "score"], axis = "columns")
#shuffles dataframe (to randomize data order)
inputs.sample(frac=1).reset_index(drop=True)

X_train, X_test, y_train, y_test = train_test_split(inputs, target,test_size=0.2, random_state=0)

#========Decision tree=========
tree_vanilla = tree.DecisionTreeClassifier()
params_tree = {"max_depth": [3,4,5],   #list of parameter combinations to try
          "min_samples_leaf" : [3,4,5,6,7,8],
          "criterion": ["entropy"]
          }
warnings.filterwarnings("ignore")
tree_cv = RandomizedSearchCV(tree_vanilla, params_tree, cv=5, n_iter=8)

#=======SVM==============
svm_vanilla = SVC()
# params_svm = {'C': [1,5,10], 'gamma': ['auto'],
#   'kernel': ['rbf', 'linear'], 'class_weight':['balanced', None]}
# # warnings.filterwarnings("ignore")
# svmModel = RandomizedSearchCV(SVC(), params_svm,cv=5, n_iter=3)

#=========random forest============
randForest_vanilla = RandomForestClassifier()

# # ============Random Forest Hyperparameters============
# # Number of trees in random forest
# n_estimators = [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)]
# # Number of features to consider at every split
# max_features = ['auto', 'sqrt']
# # Maximum number of levels in tree
# max_depth = [2,4]
# # Minimum number of samples required to split a node
# min_samples_split = [2, 5]
# # Minimum number of samples required at each leaf node
# min_samples_leaf = [1, 2]
# # Method of selecting samples for training each tree
# bootstrap = [True, False]
# # Create the param grid
# param_grid = {'n_estimators': n_estimators,
#                'max_features': max_features,
#                'max_depth': max_depth,
#                'min_samples_split': min_samples_split,
#                'min_samples_leaf': min_samples_leaf,
#                'bootstrap': bootstrap}
# rf_Model = RandomForestClassifier()

# rf_Grid = GridSearchCV(estimator = rf_Model, param_grid = param_grid, cv = 3, verbose=2, n_jobs = 4)
# rf_Grid.fit(X_train, y_train)

# print(rf_Grid.best_params_)

# # Check accuracy
# print (f'Train Accuracy - : {rf_Grid.score(X_train,y_train):.3f}')
# print (f'Test Accuracy - : {rf_Grid.score(X_test,y_test):.3f}')
# ============END OF Random Forest Hyperparameters============

#========Gradient Boost==============
gradBoost_vanilla = GradientBoostingClassifier()
params_GB = {'learning_rate':[0.15,0.01, 1], 'n_estimators':[3,10,20,50,100]}
gb_tuned = GridSearchCV(gradBoost_vanilla, params_GB)


get_score(tree_vanilla, X_train, X_test, y_train, y_test)
get_score(tree_cv, X_train, X_test, y_train, y_test)
get_score(svm_vanilla, X_train, X_test, y_train, y_test)
get_score(randForest_vanilla, X_train, X_test, y_train, y_test)
get_score(gb_tuned, X_train, X_test, y_train, y_test)
