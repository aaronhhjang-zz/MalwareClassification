import warnings
import pandas as pd
import os
import glob
import logging
import pickle

from sklearn.preprocessing import LabelEncoder
from sklearn import tree
import dataPreprocessing
from sklearn.model_selection import train_test_split

path_to_json = r".\simplifiedReports\\"

# Inputs & Target dataframes persisted in respective pkl files.
# Delete the files if data changes and preprocessing should be rerun.
inputs_pkl_filename = "inputs-NN.pkl"
target_pkl_filenmae = "target-NN.pkl"

# Configure logging
logging.basicConfig(filename="results-NN.log",
                            filemode='a',
                            format='%(asctime)s %(levelname)s %(message)s',
                            datefmt="%Y-%m-%d %H:%M:%S",
                            level=logging.DEBUG)
logging.info("==================================================================") # Separation for each line

def get_score(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train) 
    score = model.score(X_test, y_test)
    logging.info("SCORE OF " + str(model) + ": " + str(score))
    if(hasattr(model, "best_params_")):
        logging.debug("=======(values from best_params and best_score)=======")
        logging.info("Tuned Params {}".format(model.best_params_))
        logging.info("Best score is {}".format(model.best_score_))
    return score

def preprocess_data():
    # Read all json files into pandas dataframe
    json_pattern = os.path.join(path_to_json,'*.json')
    file_list = glob.glob(json_pattern)
    dfs = [] # an empty list to store the data frames
    for file in file_list:
        data = pd.read_json(file, lines=True) # read data frame from json file
        dfs.append(data) # append the data frame to the list
    df = pd.concat(dfs, ignore_index=True) # concatenate all the data frames in the list.

    #making "inputs" dataframe
    #split data into inputs and target (X and Y)
    inputs = df.drop(['name','signatures','family'], axis='columns')
    #inputs = inputs.drop('family', axis='columns')
    target = df['family']

    index = 0
    for feature in dataPreprocessing.FEATURES:
        inputs[feature] = dataPreprocessing.transposedFeatureMatrix[index]
        index += 1

    # Encode data
    # le_score = LabelEncoder()
    # inputs['scoreNEW'] = le_score.fit_transform(inputs['score'])
    # inputs = inputs.drop(["score"], axis = "columns")
    #shuffles dataframe (to randomize data order)
    inputs.sample(frac=1).reset_index(drop=True)
    return inputs, target

try:
    inputs = pd.read_pickle(inputs_pkl_filename)
    target = pd.read_pickle(target_pkl_filenmae)
    print(inputs)
except:
    print('Data not found. Need to run preprocess input and target...')
    inputs, target = preprocess_data()
    inputs.to_pickle(inputs_pkl_filename)
    target.to_pickle(target_pkl_filenmae)

X_train, X_test, y_train, y_test = train_test_split(inputs, target,test_size=0.2, random_state=0)

#========Decision tree=========
tree_vanilla = tree.DecisionTreeClassifier()

#==============training of the models and retrieving their score=================
get_score(tree_vanilla, X_train, X_test, y_train, y_test)

with open("model_pickle", "wb") as f:
    pickle.dump(tree_vanilla,f)



